{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 00 Bulk Data Acquisition\n\nOne-click pipeline to fetch all transcriptomic, proteomic, metabolomic, and reference assets for the NETs project. Edit the master configuration first, then execute the cells from top to bottom in Google Colab.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# === MASTER CONFIGURATION ===\n# 1. 基础路径\nPROJECT_DIR = \"/content/drive/MyDrive/NetsAnalysisProject\"\nRAW_DATA_DIR = f\"{PROJECT_DIR}/data/raw\"\n\n# 2. 转录组数据 (GEO accession IDs)\nGEO_IDS = [\n    \"GSE184869\",\n    \"GSE125989\",\n    \"GSE43837\",\n    \"GSE14017\",\n    \"GSE14018\",\n]\n\n# 3. 蛋白质组数据 (PRIDE accession IDs)\nPRIDE_IDS = [\n    \"PXD011796\",  # NET 参考蛋白组\n    \"PXD005719\",  # 脑转移膜蛋白组\n    \"PXD046330\",  # HER2+ 脑分泌因子模型\n    \"PXD051579\",  # HER2+ 脑转移BBB模型\n]\n\n# 4. 代谢组数据 (通过 URL 清单文件下载)\nMETABOLOMICS_MANIFEST = f\"{PROJECT_DIR}/resources/manifests/metabolomics_urls.txt\"\n\n# 5. 参考数据 (例如人类蛋白组 FASTA)\nREFERENCE_URLS = [\n    \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/reference_proteomes/Eukaryota/UP000005640/UP000005640_9606.fasta.gz\",\n]\n\n# 6. 下载清单输出位置 (自动创建)\nDOWNLOAD_LIST_DIR = f\"{PROJECT_DIR}/tmp\"\nARIA2_PRIDE_LIST = f\"{DOWNLOAD_LIST_DIR}/aria2c_pride_urls.txt\"\nARIA2_GEO_LIST = f\"{DOWNLOAD_LIST_DIR}/aria2c_geo_urls.txt\"\nARIA2_REFERENCE_LIST = f\"{DOWNLOAD_LIST_DIR}/aria2c_reference_urls.txt\"\nMASTER_DOWNLOAD_LIST = f\"{DOWNLOAD_LIST_DIR}/MASTER_DOWNLOAD_LIST.txt\"\n\n# 7. 下载控制\nSKIP_EXISTING = True  # Skip already-downloaded files when building master list\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport os\nimport sys\nimport json\nimport shutil\nimport subprocess\nfrom pathlib import Path\nfrom typing import Iterable, List, Tuple\nfrom urllib.parse import urlparse, unquote\n\nimport requests\n\nprint(f\"requests version: {requests.__version__}\")\n\nPROJECT_DIR_PATH = Path(PROJECT_DIR)\nRAW_DATA_PATH = Path(RAW_DATA_DIR)\nDOWNLOAD_LIST_PATH = Path(DOWNLOAD_LIST_DIR)\nTRANSCRIPTOMICS_DIR = RAW_DATA_PATH / \"transcriptomics\"\nPROTEOMICS_DIR = RAW_DATA_PATH / \"proteomics\"\nMETABOLOMICS_DIR = RAW_DATA_PATH / \"metabolomics\"\nREFERENCE_DIR = RAW_DATA_PATH / \"reference\"\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef ensure_dir(path: Path) -> Path:\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n\ndef infer_geo_series_folder(geo_id: str) -> str:\n    if not geo_id.startswith(\"GSE\"):\n        raise ValueError(f\"Unexpected GEO accession format: {geo_id}\")\n    numeric = geo_id[3:]\n    if len(numeric) < 3:\n        raise ValueError(f\"GEO accession is too short: {geo_id}\")\n    head = numeric[:-3]\n    return f\"GSE{head}nnn\"\n\ndef write_aria2_block(handle, target_dir: Path, filename: str, url: str) -> None:\n    handle.write(f\"dir={target_dir}\\n\")\n    handle.write(f\"out={filename}\\n\")\n    handle.write(f\"{url}\\n\\n\")\n\ndef generate_pride_urls(pxd_ids: Iterable[str], output_path: str, categories: Iterable[str] = (\"RAW\", \"RESULT\")) -> Tuple[Path, int]:\n    session = requests.Session()\n    allowed = {c.upper() for c in categories}\n    output = Path(output_path)\n    ensure_dir(output.parent)\n    ensure_dir(PROTEOMICS_DIR)\n    total = 0\n    with output.open(\"w\", encoding=\"utf-8\") as handle:\n        handle.write(\"# PRIDE download manifest\\n\")\n        for accession in pxd_ids:\n            accession = accession.strip()\n            if not accession:\n                continue\n            dataset_dir = ensure_dir(PROTEOMICS_DIR / accession)\n            handle.write(f\"# {accession}\\n\")\n            offset = 0\n            limit = 200\n            while True:\n                url = f\"https://www.ebi.ac.uk/pride/ws/archive/v2/projects/{accession}/files\"\n                resp = session.get(url, params={\"offset\": offset, \"limit\": limit}, timeout=60)\n                if resp.status_code == 404:\n                    print(f\"[warn] {accession} not found on PRIDE API\")\n                    break\n                resp.raise_for_status()\n                payload = resp.json()\n                if not payload:\n                    break\n                for entry in payload:\n                    category = (entry.get(\"fileCategory\") or {}).get(\"value\", \"\").upper()\n                    if allowed and category not in allowed:\n                        continue\n                    ftp_locations = [loc for loc in entry.get(\"publicFileLocations\", []) if loc.get(\"name\") == \"FTP Protocol\"]\n                    if not ftp_locations:\n                        continue\n                    ftp_url = ftp_locations[0][\"value\"]\n                    filename = entry.get(\"fileName\") or Path(urlparse(ftp_url).path).name\n                    if not filename:\n                        continue\n                    write_aria2_block(handle, dataset_dir, filename, ftp_url)\n                    total += 1\n                if len(payload) < limit:\n                    break\n                offset += limit\n            handle.write(\"\\n\")\n    print(f\"[generate_pride_urls] {total} files written to {output}\")\n    return output, total\n\ndef generate_geo_urls(geo_ids: Iterable[str], output_path: str) -> Tuple[Path, int]:\n    output = Path(output_path)\n    ensure_dir(output.parent)\n    ensure_dir(TRANSCRIPTOMICS_DIR)\n    total = 0\n    with output.open(\"w\", encoding=\"utf-8\") as handle:\n        handle.write(\"# GEO download manifest\\n\")\n        for geo_id in geo_ids:\n            geo_id = geo_id.strip()\n            if not geo_id:\n                continue\n            series_folder = infer_geo_series_folder(geo_id)\n            base_url = f\"ftp://ftp.ncbi.nlm.nih.gov/geo/series/{series_folder}/{geo_id}\"\n            dataset_dir = ensure_dir(TRANSCRIPTOMICS_DIR / geo_id)\n            handle.write(f\"# {geo_id}\\n\")\n            if geo_id == \"GSE184869\":\n                excel_filename = \"GSE184869_rna_seq_batch_corrected_log2_TMM_normalised_CPM_protein_coding_genes.xlsx\"\n                excel_url = f\"{base_url}/suppl/{excel_filename}\"\n                write_aria2_block(handle, dataset_dir, excel_filename, excel_url)\n                total += 1\n                handle.write(\"\\n\")\n                continue\n            matrix_url = f\"{base_url}/matrix/{geo_id}_series_matrix.txt.gz\"\n            write_aria2_block(handle, dataset_dir, f\"{geo_id}_series_matrix.txt.gz\", matrix_url)\n            total += 1\n            raw_url = f\"{base_url}/suppl/{geo_id}_RAW.tar\"\n            write_aria2_block(handle, dataset_dir, f\"{geo_id}_RAW.tar\", raw_url)\n            total += 1\n            handle.write(\"\\n\")\n    print(f\"[generate_geo_urls] {total} files written to {output}\")\n    return output, total\n\ndef derive_metabolomics_filename(url: str, counter: int) -> str:\n    parsed = urlparse(url)\n    path_name = Path(parsed.path).name\n    if path_name and path_name.lower() != \"study_download.php\":\n        return unquote(path_name)\n    params = {}\n    if parsed.query:\n        for chunk in parsed.query.split('&'):\n            if '=' in chunk:\n                k, v = chunk.split('=', 1)\n                params.setdefault(k.upper(), v)\n    study = params.get('STUDY_ID', f'STUDY{counter:03d}')\n    analysis = params.get('ANALYSIS_ID', f'AN{counter:03d}')\n    data_type = params.get('DATA_TYPE', 'data').replace('/', '_')\n    return f\"{study}_{analysis}_{data_type}.zip\"\n\ndef generate_reference_urls(urls: Iterable[str], output_path: str) -> Tuple[Path, int]:\n    output = Path(output_path)\n    ensure_dir(output.parent)\n    ensure_dir(REFERENCE_DIR)\n    total = 0\n    with output.open(\"w\", encoding=\"utf-8\") as handle:\n        handle.write(\"# Reference assets\\n\")\n        for idx, url in enumerate(urls, start=1):\n            url = url.strip()\n            if not url:\n                continue\n            parsed = urlparse(url)\n            filename = Path(parsed.path).name\n            if not filename:\n                filename = f\"reference_asset_{idx}.dat\"\n            filename = unquote(filename)\n            write_aria2_block(handle, REFERENCE_DIR, filename, url)\n            total += 1\n        handle.write(\"\\n\")\n    print(f\"[generate_reference_urls] {total} files written to {output}\")\n    return output, total\n\ndef merge_manifests(manifests: List[Path], metabolomics_manifest: Path, master_path: Path, skip_existing: bool = True) -> Tuple[Path, int, int]:\n    ensure_dir(master_path.parent)\n    ensure_dir(METABOLOMICS_DIR)\n    queued_entries = 0\n    skipped_entries = 0\n    skipped_paths: List[Path] = []\n\n    def maybe_write(target_dir: Path, filename: str, url: str) -> None:\n        nonlocal queued_entries, skipped_entries\n        dest = Path(target_dir) / filename\n        if skip_existing and dest.exists() and dest.stat().st_size > 0:\n            skipped_entries += 1\n            skipped_paths.append(dest)\n            return\n        write_aria2_block(master, target_dir, filename, url)\n        queued_entries += 1\n\n    with master_path.open(\"w\", encoding=\"utf-8\") as master:\n        master.write(\"# MASTER DOWNLOAD LIST\\n\\n\")\n        for manifest in manifests:\n            master.write(f\"# include {manifest.name}\\n\")\n            current_dir: Path = RAW_DATA_PATH\n            pending_out: str | None = None\n            with manifest.open(\"r\", encoding=\"utf-8\") as handle:\n                for raw_line in handle:\n                    line = raw_line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    if line.startswith('dir='):\n                        current_dir = Path(line.split('=', 1)[1])\n                        continue\n                    if line.startswith('out='):\n                        pending_out = line.split('=', 1)[1]\n                        continue\n                    if pending_out is None:\n                        continue\n                    maybe_write(current_dir, pending_out, line)\n                    pending_out = None\n            master.write(\"\\n\")\n        master.write(\"# metabolomics manifest\\n\")\n        count = 0\n        with metabolomics_manifest.open(\"r\", encoding=\"utf-8\") as handle:\n            for raw_line in handle:\n                line = raw_line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                count += 1\n                filename = derive_metabolomics_filename(line, count)\n                maybe_write(METABOLOMICS_DIR, filename, line)\n    print(f\"[merge_manifests] Queued {queued_entries} downloads (skipped {skipped_entries}). -> {master_path}\")\n    if skipped_paths:\n        preview = ', '.join(str(p.relative_to(RAW_DATA_PATH)) for p in skipped_paths[:5])\n        if len(skipped_paths) > 5:\n            preview += ', ...'\n        print(f\"[merge_manifests] Skipped existing files: {preview}\")\n    return master_path, queued_entries, skipped_entries\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Step 1: Environment preparation\ntry:\n    import google.colab  # type: ignore\n    from google.colab import drive  # type: ignore\n    if not Path(\"/content/drive\").exists() or not list(Path(\"/content/drive\").glob(\"MyDrive\")):\n        drive.mount(\"/content/drive\")\n    else:\n        print(\"[info] Google Drive already mounted.\")\nexcept ModuleNotFoundError:\n    print(\"[info] google.colab package not available. Skipping automatic mount.\")\n\nfor directory in [PROJECT_DIR_PATH, RAW_DATA_PATH, DOWNLOAD_LIST_PATH, TRANSCRIPTOMICS_DIR, PROTEOMICS_DIR, METABOLOMICS_DIR, REFERENCE_DIR]:\n    ensure_dir(directory)\n    print(f\"[mkdir] {directory}\")\n\nif shutil.which(\"aria2c\") is None:\n    print(\"[setup] Installing aria2...\")\n    subprocess.run([\"apt-get\", \"update\"], check=True)\n    subprocess.run([\"apt-get\", \"install\", \"-y\", \"aria2\"], check=True)\nelse:\n    print(\"[info] aria2c already available.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Step 2: Generate download manifests\ndownload_manifests = []\npride_path, pride_count = generate_pride_urls(PRIDE_IDS, ARIA2_PRIDE_LIST)\ndownload_manifests.append(pride_path)\ngeo_path, geo_count = generate_geo_urls(GEO_IDS, ARIA2_GEO_LIST)\ndownload_manifests.append(geo_path)\nreference_path, reference_count = generate_reference_urls(REFERENCE_URLS, ARIA2_REFERENCE_LIST)\ndownload_manifests.append(reference_path)\n\nmetabolomics_manifest_path = Path(METABOLOMICS_MANIFEST)\nif not metabolomics_manifest_path.exists():\n    raise FileNotFoundError(f\"Metabolomics manifest not found: {metabolomics_manifest_path}\")\nmet_lines = sum(1 for line in metabolomics_manifest_path.open(\"r\", encoding=\"utf-8\") if line.strip() and not line.strip().startswith(\"#\"))\nprint(f\"[info] metabolomics manifest ready: {metabolomics_manifest_path} ({met_lines} URLs)\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Step 3: Merge manifests and run bulk download\nmaster_path, queued_count, skipped_count = merge_manifests(\n    download_manifests,\n    Path(METABOLOMICS_MANIFEST),\n    Path(MASTER_DOWNLOAD_LIST),\n    skip_existing=SKIP_EXISTING,\n)\nprint(f\"[info] MASTER_DOWNLOAD_LIST: {master_path}\")\nprint(f\"[info] Downloads queued this run: {queued_count}\")\nprint(f\"[info] Existing files skipped: {skipped_count}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Execute single aria2c command (may take a long time)\nprint(\"--- Starting Bulk Download ---\")\naria2_cmd = [\n    \"aria2c\",\n    \"-c\",\n    \"-x\", \"16\",\n    \"-s\", \"16\",\n    \"--max-tries=0\",\n    \"--retry-wait=30\",\n    f\"--input-file={MASTER_DOWNLOAD_LIST}\",\n    f\"--dir={RAW_DATA_DIR}\",\n    f\"--log={RAW_DATA_DIR}/download.log\",\n    \"--log-level=info\",\n]\nresult = subprocess.run(aria2_cmd, check=False)\nprint(f\"[aria2c] return code: {result.returncode}\")\nprint(\"--- Bulk Download Command Issued. Check download.log for progress. ---\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Step 3b: Reorganize downloaded files into target directories\nfrom collections import defaultdict\n\ndef parse_master_manifest(manifest_path: Path) -> List[Tuple[Path, str]]:\n    mapping = []\n    current_dir = RAW_DATA_PATH\n    with manifest_path.open('r', encoding='utf-8') as handle:\n        for raw_line in handle:\n            line = raw_line.strip()\n            if not line or line.startswith('#'):\n                continue\n            if line.startswith('dir='):\n                current_dir = Path(line.split('=', 1)[1])\n                continue\n            if line.startswith('out='):\n                filename = line.split('=', 1)[1]\n                mapping.append((current_dir, filename))\n                continue\n    return mapping\n\ndef locate_candidate(filename: str, dest_path: Path) -> Path | None:\n    direct = RAW_DATA_PATH / filename\n    if direct.exists():\n        return direct\n    alt = RAW_DATA_PATH / f\"{filename}.1\"\n    if alt.exists():\n        return alt\n    stem, suffix = Path(filename).stem, ''.join(Path(filename).suffixes)\n    if suffix:\n        dotted = RAW_DATA_PATH / f\"{stem}.1{suffix}\"\n        if dotted.exists():\n            return dotted\n    matches = [p for p in RAW_DATA_PATH.glob(f\"{filename}*\") if p != dest_path]\n    if matches:\n        return matches[0]\n    return None\n\ndef ensure_parent(path: Path) -> None:\n    ensure_dir(path.parent)\n\nmanifest_path = Path(MASTER_DOWNLOAD_LIST)\nif not manifest_path.exists():\n    raise FileNotFoundError(f\"MASTER_DOWNLOAD_LIST not found: {manifest_path}\")\n\nentries = parse_master_manifest(manifest_path)\nmove_records = []\nmissing_entries = []\nalready_correct = 0\n\nfor target_dir, filename in entries:\n    dest_path = Path(target_dir) / filename\n    if dest_path.exists():\n        already_correct += 1\n        continue\n    candidate = locate_candidate(filename, dest_path)\n    if candidate and candidate.exists():\n        ensure_parent(dest_path)\n        shutil.move(str(candidate), str(dest_path))\n        move_records.append((candidate, dest_path))\n    else:\n        missing_entries.append((filename, target_dir))\n\nprint(f\"[organize] Already in place: {already_correct}\")\nprint(f\"[organize] Moved files: {len(move_records)}\")\nfor src, dest in move_records[:10]:\n    print(f\"  moved -> {dest.relative_to(RAW_DATA_PATH)}\")\nif len(move_records) > 10:\n    print(f\"  ... {len(move_records) - 10} more\")\n\nif missing_entries:\n    print(f\"[organize] Missing files: {len(missing_entries)}\")\n    for fname, tdir in missing_entries[:10]:\n        print(f\"  missing {fname} -> {tdir}\")\n    if len(missing_entries) > 10:\n        print(f\"  ... {len(missing_entries) - 10} more\")\nelse:\n    print(\"[organize] No missing files detected.\")\n\nleftover = [p for p in RAW_DATA_PATH.iterdir() if p.is_file()]\nif leftover:\n    print(f\"[organize] Unassigned files remaining in {RAW_DATA_PATH}: {len(leftover)}\")\n    for path in leftover[:10]:\n        size = path.stat().st_size\n        print(f\"  leftover {path.name} (size={size})\")\n    if len(leftover) > 10:\n        print(f\"  ... {len(leftover) - 10} more\")\nelse:\n    print(\"[organize] No files left in the top-level raw directory.\")\n\nzero_bytes = []\nfor target_dir, filename in entries:\n    dest_path = Path(target_dir) / filename\n    if dest_path.exists() and dest_path.stat().st_size == 0:\n        zero_bytes.append(dest_path)\nif zero_bytes:\n    print(f\"[organize] Warning: {len(zero_bytes)} zero-byte files detected (failed downloads):\")\n    for path in zero_bytes[:10]:\n        print(f\"  zero {path.relative_to(RAW_DATA_PATH)}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Step 4: Post-download verification\nsubprocess.run([\"ls\", \"-lR\", RAW_DATA_DIR], check=False)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}