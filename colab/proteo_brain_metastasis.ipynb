{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e4f163",
   "metadata": {},
   "source": [
    "\n",
    "# Proteomics Pipeline: Brain Metastasis Cohorts (PXD005719, PXD046330, PXD051579)\n",
    "\n",
    "Batch Colab workflow that reuses shared tooling (msconvert, MSFragger, Philosopher) to generate log2 protein abundance matrices and metadata for the brain-metastasis related datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Master Configuration ===\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/NetsAnalysisProject\"\n",
    "DATASETS = [\n",
    "    \"PXD005719\",  # membrane/BBB associated proteome\n",
    "    \"PXD046330\",  # HER2+ conditioned media / secretome\n",
    "    \"PXD051579\",  # BBB dysfunction time-course\n",
    "]\n",
    "RAW_PROTEO_DIR = f\"{PROJECT_DIR}/data/raw/proteomics\"\n",
    "REFERENCE_DIR = f\"{PROJECT_DIR}/data/raw/reference\"\n",
    "OUTPUT_ROOT = f\"{PROJECT_DIR}/data/processed/proteomics\"\n",
    "MZML_ROOT = f\"{OUTPUT_ROOT}/mzml\"\n",
    "WORK_ROOT = f\"{PROJECT_DIR}/tmp/brain_proteomics\"\n",
    "TOOLS_DIR = f\"{PROJECT_DIR}/tools\"\n",
    "MSCONVERT_BIN = f\"{TOOLS_DIR}/pwiz/msconvert\"\n",
    "MSFRAGGER_JAR = f\"{TOOLS_DIR}/msfragger/MSFragger-3.7.jar\"\n",
    "PHILOSOPHER_BIN = f\"{TOOLS_DIR}/philosopher/philosopher\"\n",
    "FASTA_GZ = f\"{REFERENCE_DIR}/UP000005640_9606.fasta.gz\"\n",
    "FASTA_PATH = f\"{REFERENCE_DIR}/UP000005640_9606.fasta\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a468b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mount Drive and ensure directories\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "except ModuleNotFoundError:\n",
    "    print('[info] Running outside Colab; ensure PROJECT_DIR is accessible.')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    Path(f\"{RAW_PROTEO_DIR}/{dataset}\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"{OUTPUT_ROOT}/{dataset}\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"{MZML_ROOT}/{dataset}\").mkdir(parents=True, exist_ok=True)\n",
    "Path(WORK_ROOT).mkdir(parents=True, exist_ok=True)\n",
    "Path(TOOLS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[setup] Datasets: {DATASETS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b131e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure toolchain is in place (assumes proteo_net_reference.ipynb has run once)\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "missing = []\n",
    "if not Path(MSCONVERT_BIN).exists():\n",
    "    missing.append('ProteoWizard msconvert')\n",
    "if not Path(MSFRAGGER_JAR).exists():\n",
    "    missing.append('MSFragger jar')\n",
    "if not Path(PHILOSOPHER_BIN).exists():\n",
    "    missing.append('Philosopher binary')\n",
    "if missing:\n",
    "    print('[warn] Missing tools:', ', '.join(missing))\n",
    "    print('Run colab/proteo_net_reference.ipynb tool-install cell first or re-run the install block here.')\n",
    "else:\n",
    "    print('[check] All tools detected.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743351c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shared helper functions\n",
    "import subprocess\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def run_cmd(cmd: List[str], cwd: Path | None = None):\n",
    "    print('[cmd]', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True, cwd=str(cwd) if cwd else None)\n",
    "\n",
    "\n",
    "def ensure_fasta() -> Path:\n",
    "    fasta_path = Path(FASTA_PATH)\n",
    "    if fasta_path.exists():\n",
    "        return fasta_path\n",
    "    gz_path = Path(FASTA_GZ)\n",
    "    if not gz_path.exists():\n",
    "        raise FileNotFoundError(f\"FASTA gz not found: {gz_path}\")\n",
    "    with gzip.open(gz_path, 'rt') as fin, open(fasta_path, 'w') as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "    print(f\"[fasta] Decompressed {gz_path} -> {fasta_path}\")\n",
    "    return fasta_path\n",
    "\n",
    "\n",
    "def convert_raw_to_mzml(dataset: str):\n",
    "    raw_dir = Path(RAW_PROTEO_DIR) / dataset\n",
    "    mzml_dir = Path(MZML_ROOT) / dataset\n",
    "    mzml_dir.mkdir(parents=True, exist_ok=True)\n",
    "    raw_files = sorted(raw_dir.glob('*.raw'))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f\"No RAW files found for {dataset}\")\n",
    "    for raw in raw_files:\n",
    "        out_file = mzml_dir / (raw.stem + '.mzML')\n",
    "        if out_file.exists():\n",
    "            print(f\"[skip] {dataset}: {out_file.name} already present\")\n",
    "            continue\n",
    "        run_cmd([\n",
    "            MSCONVERT_BIN,\n",
    "            str(raw),\n",
    "            '--mzML',\n",
    "            '--filter', 'peakPicking true 1-',\n",
    "            '--outfile', out_file.name,\n",
    "            '--outdir', str(mzml_dir)\n",
    "        ])\n",
    "    print(f\"[msconvert] {dataset}: {len(raw_files)} files processed\")\n",
    "\n",
    "\n",
    "def write_msfragger_params(dataset: str, fasta_path: Path) -> Path:\n",
    "    params_dir = Path(WORK_ROOT) / dataset\n",
    "    params_dir.mkdir(parents=True, exist_ok=True)\n",
    "    params_path = params_dir / 'msfragger.params'\n",
    "    params_path.write_text(\n",
    "        f'''num_threads = 16\n",
    "precursor_true_tolerance = 15\n",
    "enzyme = trypsin\n",
    "isotope_error = 0/1/2\n",
    "allowed_missed_cleavage = 2\n",
    "fragment_bin_tol = 0.02\n",
    "precursor_mass_lower = -20\n",
    "precursor_mass_upper = 20\n",
    "precursor_mass_units = ppm\n",
    "fragment_mass_units = Dalton\n",
    "search_enzyme_name = trypsin\n",
    "protein_database = {fasta_path}\n",
    "add_Cterm_peptide = 0.0\n",
    "add_Nterm_peptide = 0.0\n",
    "add_G_glycine = 0.0\n",
    "add_C_cysteine = 57.021464\n",
    "variable_mod_01 = 15.994915 M 100.0\n",
    "variable_mod_02 = 15.994915 P 10.0\n",
    "variable_mod_03 = 0.984016 NQ 1.0\n",
    "localize_delta_mass = 1\n",
    "replace_missing = 1\n",
    "zero_bin_accept_expect = 0\n",
    "remove_precursor_peak = 1\n",
    "isotope_error_correction = 1\n",
    "calculate_pq_peptide_probability = 1\n",
    "'''\n",
    "    )\n",
    "    return params_path\n",
    "\n",
    "\n",
    "def run_msfragger(dataset: str, params_path: Path):\n",
    "    mzml_files = sorted((Path(MZML_ROOT) / dataset).glob('*.mzML'))\n",
    "    if not mzml_files:\n",
    "        raise FileNotFoundError(f'No mzML files for {dataset}')\n",
    "    cmd = ['java', '-Xmx24G', '-jar', MSFRAGGER_JAR, str(params_path)] + [str(f) for f in mzml_files]\n",
    "    run_cmd(cmd, cwd=params_path.parent)\n",
    "\n",
    "\n",
    "def run_philosopher(dataset: str, fasta_path: Path) -> Path:\n",
    "    work_dir = Path(WORK_ROOT) / dataset\n",
    "    workspace = work_dir / 'philosopher'\n",
    "    workspace.mkdir(parents=True, exist_ok=True)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'workspace', '--init'], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'database', '--annotate', str(fasta_path)], cwd=workspace)\n",
    "\n",
    "    pepxml_files = sorted(work_dir.glob('*.pepXML')) or sorted(work_dir.glob('*.pep.xml'))\n",
    "    if not pepxml_files:\n",
    "        raise FileNotFoundError(f'No pepXML files for {dataset}')\n",
    "\n",
    "    for pepxml in pepxml_files:\n",
    "        run_cmd([PHILOSOPHER_BIN, 'peptideprophet', '--ppm', '--nonparam', '--expectscore', str(pepxml)], cwd=workspace)\n",
    "\n",
    "    interact_files = sorted(workspace.glob('interact-*.pep.xml'))\n",
    "    run_cmd([PHILOSOPHER_BIN, 'proteinprophet'] + [str(f) for f in interact_files], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'filter', '--sequential', '--picked', '--peptide', '--prot'], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'label-free'] + [str(f) for f in interact_files], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'report', '--msstats'], cwd=workspace)\n",
    "    return workspace\n",
    "\n",
    "\n",
    "def load_protein_table(workspace: Path) -> pd.DataFrame:\n",
    "    candidates = list(workspace.glob('*.protein.tsv'))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError('Protein report missing')\n",
    "    return pd.read_csv(candidates[0], sep='\t')\n",
    "\n",
    "\n",
    "def uniprot_to_gene(uniprot_ids: List[str]) -> pd.DataFrame:\n",
    "    ids = [uid for uid in set(uniprot_ids) if isinstance(uid, str) and uid]\n",
    "    if not ids:\n",
    "        return pd.DataFrame(columns=['UniProt', 'Gene'])\n",
    "    job = requests.post('https://rest.uniprot.org/idmapping/run', data={'from': 'UniProtKB_AC-ID', 'to': 'Gene_Name', 'ids': ','.join(ids)}).json()\n",
    "    job_id = job['jobId']\n",
    "    status_url = f'https://rest.uniprot.org/idmapping/status/{job_id}'\n",
    "    result_url = f'https://rest.uniprot.org/idmapping/stream/{job_id}'\n",
    "    while True:\n",
    "        status = requests.get(status_url).json()\n",
    "        if status.get('jobStatus') == 'FINISHED':\n",
    "            break\n",
    "        if status.get('jobStatus') == 'FAILED':\n",
    "            raise RuntimeError('UniProt mapping failed')\n",
    "        time.sleep(3)\n",
    "    rows = []\n",
    "    for entry in requests.get(result_url).json().get('results', []):\n",
    "        gene = entry.get('to')\n",
    "        if gene:\n",
    "            rows.append({'UniProt': entry['from'], 'Gene': gene.split(';')[0]})\n",
    "    return pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "\n",
    "def build_matrix(dataset: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    output_dir = Path(OUTPUT_ROOT) / dataset\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    intensity_cols = [c for c in df.columns if 'Intensity' in c]\n",
    "    if not intensity_cols:\n",
    "        raise ValueError(f'No intensity columns detected in protein report for {dataset}')\n",
    "    df = df.copy()\n",
    "    df['PrimaryAcc'] = df['Protein'].str.split(';').str[0]\n",
    "    mapping = uniprot_to_gene(df['PrimaryAcc'].tolist())\n",
    "    df = df.merge(mapping, how='left', left_on='PrimaryAcc', right_on='UniProt')\n",
    "    df['Gene'] = df['Gene'].fillna(df['PrimaryAcc'])\n",
    "    df = df.drop_duplicates(subset=['Gene']).set_index('Gene')\n",
    "    matrix = np.log2(df[intensity_cols].replace(0, np.nan))\n",
    "    matrix_path = output_dir / 'protein_abundance.tsv'\n",
    "    matrix.to_csv(matrix_path, sep='\t')\n",
    "    map_path = output_dir / 'uniprot_to_hgnc.tsv'\n",
    "    if not mapping.empty:\n",
    "        mapping.to_csv(map_path, sep='\t', index=False)\n",
    "    else:\n",
    "        df[['PrimaryAcc']].rename(columns={'PrimaryAcc': 'UniProt'}).assign(Gene=df.index).to_csv(map_path, sep='\t', index=False)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def derive_metadata(dataset: str, matrix: pd.DataFrame) -> pd.DataFrame:\n",
    "    samples = matrix.columns\n",
    "    records = []\n",
    "    for sample in samples:\n",
    "        clean = sample.replace('.raw', '').replace('.mzML', '')\n",
    "        tokens = clean.split('_')\n",
    "        treatment = 'NA'\n",
    "        organ = 'Brain'\n",
    "        replicate = 'NA'\n",
    "        for token in tokens:\n",
    "            if token.lower() in {'control', 'treatment', 'treated'}:\n",
    "                treatment = token.lower()\n",
    "            if token.lower() in {'br', 'brain'}:\n",
    "                organ = 'Brain'\n",
    "            if token.lower() in {'p', 'parental'}:\n",
    "                organ = 'Parental'\n",
    "            if token.lower().startswith('rep'):\n",
    "                replicate = token.lower()\n",
    "        records.append({'sample_id': sample, 'treatment': treatment, 'organ': organ, 'replicate': replicate})\n",
    "    meta = pd.DataFrame(records)\n",
    "    meta_path = Path(OUTPUT_ROOT) / dataset / 'metadata.tsv'\n",
    "    meta.to_csv(meta_path, sep='\t', index=False)\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ce4b3",
   "metadata": {},
   "source": [
    "### Batch execution across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "fasta_path = ensure_fasta()\n",
    "summary = []\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    print(f\"==== Processing {dataset} ====\")\n",
    "    convert_raw_to_mzml(dataset)\n",
    "    params = write_msfragger_params(dataset, fasta_path)\n",
    "    run_msfragger(dataset, params)\n",
    "    workspace = run_philosopher(dataset, fasta_path)\n",
    "    protein_df = load_protein_table(workspace)\n",
    "    matrix = build_matrix(dataset, protein_df)\n",
    "    meta = derive_metadata(dataset, matrix)\n",
    "    summary.append({\n",
    "        'dataset': dataset,\n",
    "        'proteins': matrix.shape[0],\n",
    "        'samples': matrix.shape[1]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc03f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Outputs:')\n",
    "for dataset in DATASETS:\n",
    "    out_dir = Path(OUTPUT_ROOT) / dataset\n",
    "    print(f\"- {dataset}: {out_dir}/protein_abundance.tsv, {out_dir}/metadata.tsv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
