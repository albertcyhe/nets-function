{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5aafb4",
   "metadata": {},
   "source": [
    "\n",
    "# Proteomics Pipeline: PXD011796 (NET Reference)\n",
    "\n",
    "End-to-end Colab workflow to convert RAW files, run MSFragger/Philosopher, and export log2 protein abundance matrices for the NET reference dataset (PXD011796).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88daead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Master Configuration ===\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/NetsAnalysisProject\"\n",
    "RAW_PROTEO_DIR = f\"{PROJECT_DIR}/data/raw/proteomics\"\n",
    "REFERENCE_DIR = f\"{PROJECT_DIR}/data/raw/reference\"\n",
    "PXD_ID = \"PXD011796\"\n",
    "RAW_INPUT_DIR = f\"{RAW_PROTEO_DIR}/{PXD_ID}\"\n",
    "OUTPUT_DIR = f\"{PROJECT_DIR}/data/processed/proteomics/{PXD_ID}\"\n",
    "MZML_DIR = f\"{OUTPUT_DIR}/mzml\"\n",
    "WORK_DIR = f\"{PROJECT_DIR}/tmp/{PXD_ID}\"\n",
    "TOOLS_DIR = f\"{PROJECT_DIR}/tools\"\n",
    "MSCONVERT_BIN = \"/usr/local/bin/msconvert_pwiz\"\n",
    "MSFRAGGER_JAR = Path('msfragger_jar.txt').read_text().strip() if Path('msfragger_jar.txt').exists() else ''\n",
    "PHILOSOPHER_BIN = f\"{TOOLS_DIR}/philosopher/philosopher\"\n",
    "FASTA_GZ = f\"{REFERENCE_DIR}/UP000005640_9606.fasta.gz\"\n",
    "FASTA_PATH = f\"{REFERENCE_DIR}/UP000005640_9606.fasta\"\n",
    "PROTEIN_MATRIX = f\"{OUTPUT_DIR}/protein_abundance.tsv\"\n",
    "METADATA_PATH = f\"{OUTPUT_DIR}/metadata.tsv\"\n",
    "UNIPROT_MAP_PATH = f\"{OUTPUT_DIR}/uniprot_to_hgnc.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mount Drive and ensure directories\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "except ModuleNotFoundError:\n",
    "    print('[info] Running outside Colab; ensure PROJECT_DIR is accessible.')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for path in [Path(PROJECT_DIR), Path(RAW_INPUT_DIR), Path(OUTPUT_DIR), Path(MZML_DIR), Path(WORK_DIR), Path(TOOLS_DIR)]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[setup] RAW files expected under: {RAW_INPUT_DIR}\")\n",
    "print(f\"[setup] Processed outputs -> {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cdbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tooling: micromamba + ProteoWizard shim, MSFragger (bioconda), Philosopher\n",
    "!apt-get update -qq\n",
    "!apt-get install -yqq wget aria2\n",
    "\n",
    "import subprocess\n",
    "import tarfile\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "TOOLS_DIR_PATH = Path(TOOLS_DIR)\n",
    "TOOLS_DIR_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MICROMAMBA_BIN = Path.home() / 'bin' / 'micromamba'\n",
    "ENV_NAME = 'pwiz'\n",
    "SHIM_PATH = Path('/usr/local/bin/msconvert_pwiz')\n",
    "\n",
    "\n",
    "def download(url: str, dest: Path):\n",
    "    if dest.exists():\n",
    "        print(f\"[cache] {dest.name} already present\")\n",
    "        return\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[download] {url}\")\n",
    "    with requests.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, 'wb') as fh:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    fh.write(chunk)\n",
    "\n",
    "# Install micromamba if missing\n",
    "if not MICROMAMBA_BIN.exists():\n",
    "    tarball = TOOLS_DIR_PATH / 'micromamba.tar.bz2'\n",
    "    download('https://micromamba.snakepit.net/api/micromamba/linux-64/latest', tarball)\n",
    "    (Path.home() / 'bin').mkdir(exist_ok=True)\n",
    "    with tarfile.open(tarball, 'r:bz2') as tf:\n",
    "        members = [m for m in tf.getmembers() if m.name.startswith('bin/')]\n",
    "        tf.extractall(path=Path.home(), members=members, filter='data')\n",
    "    MICROMAMBA_BIN.chmod(0o755)\n",
    "else:\n",
    "    print('[cache] micromamba already installed')\n",
    "\n",
    "# Ensure proteowizard + msfragger packages are installed\n",
    "create_cmd = [str(MICROMAMBA_BIN), 'create', '-y', '-n', ENV_NAME,\n",
    "              '-c', 'conda-forge', '-c', 'bioconda', 'proteowizard', 'msfragger']\n",
    "install_cmd = [str(MICROMAMBA_BIN), 'install', '-y', '-n', ENV_NAME,\n",
    "               '-c', 'conda-forge', '-c', 'bioconda', 'proteowizard', 'msfragger']\n",
    "ret = subprocess.run(create_cmd)\n",
    "if ret.returncode != 0:\n",
    "    subprocess.run(install_cmd, check=True)\n",
    "\n",
    "# Verify msfragger is present\n",
    "subprocess.run([str(MICROMAMBA_BIN), 'run', '-n', ENV_NAME, 'bash', '-lc', 'conda list msfragger'], check=True)\n",
    "\n",
    "# Verify msconvert availability\n",
    "subprocess.run([str(MICROMAMBA_BIN), 'run', '-n', ENV_NAME, 'msconvert', '--help'],\n",
    "               check=True, stdout=subprocess.DEVNULL)\n",
    "\n",
    "# Create shim script for msconvert\n",
    "shim_script = f\"\"\"#!/usr/bin/env bash\n",
    "set -e\n",
    "{MICROMAMBA_BIN} run -n {ENV_NAME} msconvert \"$@\"\n",
    "\"\"\"\n",
    "SHIM_PATH.write_text(shim_script)\n",
    "SHIM_PATH.chmod(0o755)\n",
    "\n",
    "# Resolve real msconvert path\n",
    "def resolve_msconvert_path() -> Path:\n",
    "    result = subprocess.run([str(MICROMAMBA_BIN), 'run', '-n', ENV_NAME, 'which', 'msconvert'],\n",
    "                            check=True, capture_output=True, text=True)\n",
    "    return Path(result.stdout.strip())\n",
    "\n",
    "msconvert_real = resolve_msconvert_path()\n",
    "\n",
    "globals()['MSCONVERT_BIN'] = str(SHIM_PATH)\n",
    "print(f\"[check] msconvert shim at {MSCONVERT_BIN}\")\n",
    "print(f\"[check] real msconvert binary at {msconvert_real}\")\n",
    "\n",
    "# Determine MSFragger jar path from environment\n",
    "conda_prefix = subprocess.check_output([\n",
    "    str(MICROMAMBA_BIN), 'run', '-n', ENV_NAME, 'bash', '-lc', 'printf %s \"$CONDA_PREFIX\"'\n",
    "], text=True).strip()\n",
    "msfragger_dir = Path(conda_prefix) / 'share' / 'msfragger'\n",
    "jar_candidates = sorted(msfragger_dir.glob('MSFragger-*.jar'))\n",
    "if not jar_candidates:\n",
    "    raise FileNotFoundError(f'MSFragger jar not found under {msfragger_dir}. Ensure msfragger package provides the jar (conda-forge/bioconda).')\n",
    "msfragger_jar_path = jar_candidates[-1]\n",
    "\n",
    "globals()['MSFRAGGER_JAR'] = str(msfragger_jar_path)\n",
    "print(f\"[check] MSFragger jar at {MSFRAGGER_JAR}\")\n",
    "\n",
    "# Philosopher installer (GitHub release tarball)\n",
    "PHILOSOPHER_PATH = Path(PHILOSOPHER_BIN)\n",
    "if not PHILOSOPHER_PATH.exists():\n",
    "    tgz_path = TOOLS_DIR_PATH / 'philosopher_v5.2.0_linux_amd64.tar.gz'\n",
    "    download('https://github.com/Nesvilab/philosopher/releases/download/v5.2.0/philosopher_v5.2.0_linux_amd64.tar.gz', tgz_path)\n",
    "    subprocess.run(['tar', '-xzf', str(tgz_path), '-C', str(TOOLS_DIR_PATH)], check=True)\n",
    "else:\n",
    "    print('[cache] Philosopher already installed')\n",
    "\n",
    "print(f\"[check] Philosopher binary found: {PHILOSOPHER_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abbc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions for proteomics workflow\n",
    "import subprocess\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def run_cmd(cmd: List[str], cwd: Path | None = None):\n",
    "    print('[cmd]', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True, cwd=str(cwd) if cwd else None)\n",
    "\n",
    "\n",
    "def ensure_fasta() -> Path:\n",
    "    fasta_path = Path(FASTA_PATH)\n",
    "    if fasta_path.exists():\n",
    "        return fasta_path\n",
    "    gz_path = Path(FASTA_GZ)\n",
    "    if not gz_path.exists():\n",
    "        raise FileNotFoundError(f\"FASTA gz not found: {gz_path}\")\n",
    "    with gzip.open(gz_path, 'rt') as fin, open(fasta_path, 'w') as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "    print(f\"[fasta] Decompressed {gz_path} -> {fasta_path}\")\n",
    "    return fasta_path\n",
    "\n",
    "\n",
    "def convert_raw_to_mzml():\n",
    "    raw_dir = Path(RAW_INPUT_DIR)\n",
    "    mzml_dir = Path(MZML_DIR)\n",
    "    mzml_dir.mkdir(parents=True, exist_ok=True)\n",
    "    raw_files = sorted(raw_dir.glob('*.raw'))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(f\"No RAW files found in {raw_dir}\")\n",
    "    for raw in raw_files:\n",
    "        out_file = mzml_dir / (raw.stem + '.mzML')\n",
    "        if out_file.exists():\n",
    "            print(f\"[skip] {out_file.name} already present\")\n",
    "            continue\n",
    "        run_cmd([\n",
    "            MSCONVERT_BIN,\n",
    "            str(raw),\n",
    "            '--mzML',\n",
    "            '--filter', 'peakPicking true 1-',\n",
    "            '--outfile', out_file.name,\n",
    "            '--outdir', str(mzml_dir)\n",
    "        ])\n",
    "    print(f\"[msconvert] Completed {len(raw_files)} conversions\")\n",
    "\n",
    "\n",
    "def write_msfragger_params(fasta_path: Path) -> Path:\n",
    "    params_path = Path(WORK_DIR) / 'msfragger.params'\n",
    "    params_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    params_path.write_text(\n",
    "        f'''num_threads = 16\n",
    "precursor_true_tolerance = 15\n",
    "enzyme = trypsin\n",
    "isotope_error = 0/1/2\n",
    "allowed_missed_cleavage = 2\n",
    "fragment_bin_tol = 0.02\n",
    "precursor_mass_lower = -20\n",
    "precursor_mass_upper = 20\n",
    "precursor_mass_units = ppm\n",
    "fragment_mass_units = Dalton\n",
    "search_enzyme_name = trypsin\n",
    "protein_database = {fasta_path}\n",
    "add_Cterm_peptide = 0.0\n",
    "add_Nterm_peptide = 0.0\n",
    "add_G_glycine = 0.0\n",
    "add_C_cysteine = 57.021464\n",
    "variable_mod_01 = 15.994915 M 100.0\n",
    "variable_mod_02 = 15.994915 P 10.0\n",
    "variable_mod_03 = 0.984016 NQ 1.0\n",
    "localize_delta_mass = 1\n",
    "replace_missing = 1\n",
    "zero_bin_accept_expect = 0\n",
    "remove_precursor_peak = 1\n",
    "isotope_error_correction = 1\n",
    "calculate_pq_peptide_probability = 1\n",
    "'''\n",
    "    )\n",
    "    return params_path\n",
    "\n",
    "\n",
    "def run_msfragger(params_path: Path):\n",
    "    mzml_files = sorted(Path(MZML_DIR).glob('*.mzML'))\n",
    "    if not mzml_files:\n",
    "        raise FileNotFoundError('No mzML files available for MSFragger')\n",
    "    cmd = ['java', '-Xmx24G', '-jar', MSFRAGGER_JAR, str(params_path)] + [str(f) for f in mzml_files]\n",
    "    run_cmd(cmd, cwd=Path(WORK_DIR))\n",
    "\n",
    "\n",
    "def run_philosopher(fasta_path: Path) -> Path:\n",
    "    workspace = Path(WORK_DIR) / 'philosopher'\n",
    "    workspace.mkdir(parents=True, exist_ok=True)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'workspace', '--init'], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'database', '--annotate', str(fasta_path)], cwd=workspace)\n",
    "\n",
    "    pepxml_files = sorted(Path(WORK_DIR).glob('*.pepXML')) or sorted(Path(WORK_DIR).glob('*.pep.xml'))\n",
    "    if not pepxml_files:\n",
    "        raise FileNotFoundError('No pepXML files produced by MSFragger')\n",
    "\n",
    "    for pepxml in pepxml_files:\n",
    "        run_cmd([PHILOSOPHER_BIN, 'peptideprophet', '--ppm', '--nonparam', '--expectscore', str(pepxml)], cwd=workspace)\n",
    "\n",
    "    interact_files = sorted(workspace.glob('interact-*.pep.xml'))\n",
    "    run_cmd([PHILOSOPHER_BIN, 'proteinprophet'] + [str(f) for f in interact_files], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'filter', '--sequential', '--picked', '--peptide', '--prot'], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'label-free'] + [str(f) for f in interact_files], cwd=workspace)\n",
    "    run_cmd([PHILOSOPHER_BIN, 'report', '--msstats'], cwd=workspace)\n",
    "    return workspace\n",
    "\n",
    "\n",
    "def load_protein_table(workspace: Path) -> pd.DataFrame:\n",
    "    candidates = list(workspace.glob('*.protein.tsv'))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError('Philosopher protein report not found')\n",
    "    return pd.read_csv(candidates[0], sep='\t')\n",
    "\n",
    "\n",
    "def uniprot_to_gene(uniprot_ids: List[str]) -> pd.DataFrame:\n",
    "    ids = [uid for uid in set(uniprot_ids) if isinstance(uid, str) and uid]\n",
    "    if not ids:\n",
    "        return pd.DataFrame(columns=['UniProt', 'Gene'])\n",
    "    job = requests.post('https://rest.uniprot.org/idmapping/run', data={\n",
    "        'from': 'UniProtKB_AC-ID',\n",
    "        'to': 'Gene_Name',\n",
    "        'ids': ','.join(ids)\n",
    "    }).json()\n",
    "    job_id = job['jobId']\n",
    "    status_url = f'https://rest.uniprot.org/idmapping/status/{job_id}'\n",
    "    result_url = f'https://rest.uniprot.org/idmapping/stream/{job_id}'\n",
    "    while True:\n",
    "        status = requests.get(status_url).json()\n",
    "        if status.get('jobStatus') == 'FINISHED':\n",
    "            break\n",
    "        if status.get('jobStatus') == 'FAILED':\n",
    "            raise RuntimeError('UniProt mapping failed')\n",
    "        time.sleep(3)\n",
    "    rows = []\n",
    "    for entry in requests.get(result_url).json().get('results', []):\n",
    "        gene = entry.get('to')\n",
    "        if gene:\n",
    "            rows.append({'UniProt': entry['from'], 'Gene': gene.split(';')[0]})\n",
    "    return pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "\n",
    "def build_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    intensity_cols = [c for c in df.columns if 'Intensity' in c]\n",
    "    if not intensity_cols:\n",
    "        raise ValueError('No intensity columns detected in protein report')\n",
    "    df = df.copy()\n",
    "    df['PrimaryAcc'] = df['Protein'].str.split(';').str[0]\n",
    "    mapping = uniprot_to_gene(df['PrimaryAcc'].tolist())\n",
    "    df = df.merge(mapping, how='left', left_on='PrimaryAcc', right_on='UniProt')\n",
    "    df['Gene'] = df['Gene'].fillna(df['PrimaryAcc'])\n",
    "    df = df.drop_duplicates(subset=['Gene']).set_index('Gene')\n",
    "    matrix = np.log2(df[intensity_cols].replace(0, np.nan))\n",
    "    matrix.to_csv(PROTEIN_MATRIX, sep='\t')\n",
    "    if not mapping.empty:\n",
    "        mapping.to_csv(UNIPROT_MAP_PATH, sep='\t', index=False)\n",
    "    else:\n",
    "        df[['PrimaryAcc']].rename(columns={'PrimaryAcc': 'UniProt'}).assign(Gene=df.index).to_csv(UNIPROT_MAP_PATH, sep='\t', index=False)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def derive_metadata(matrix: pd.DataFrame) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for sample in matrix.columns:\n",
    "        clean = sample.replace('.raw', '').replace('.mzML', '')\n",
    "        tokens = clean.split('_')\n",
    "        disease = 'Unknown'\n",
    "        stimulus = 'NA'\n",
    "        timepoint = 'NA'\n",
    "        for token in tokens:\n",
    "            upper = token.upper()\n",
    "            if upper.startswith('SLE'):\n",
    "                disease = 'SLE'\n",
    "            elif upper.startswith('RA'):\n",
    "                disease = 'RA'\n",
    "            elif upper.startswith('NC'):\n",
    "                disease = 'Healthy'\n",
    "            if upper in {'PMA', 'A23', 'A23187'}:\n",
    "                stimulus = upper\n",
    "            if upper.endswith('H') and upper[:-1].isdigit():\n",
    "                timepoint = upper\n",
    "        records.append({\n",
    "            'sample_id': sample,\n",
    "            'disease': disease,\n",
    "            'stimulus': stimulus,\n",
    "            'timepoint': timepoint\n",
    "        })\n",
    "    meta = pd.DataFrame(records)\n",
    "    meta.to_csv(METADATA_PATH, sep='\t', index=False)\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a862fa",
   "metadata": {},
   "source": [
    "### Step 1: Prepare FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_path = ensure_fasta(); fasta_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f4d82",
   "metadata": {},
   "source": [
    "### Step 2: Convert RAW files to mzML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc857d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_raw_to_mzml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160ab8a",
   "metadata": {},
   "source": [
    "### Step 3: Run MSFragger search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d17cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = write_msfragger_params(ensure_fasta()); run_msfragger(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b141ba",
   "metadata": {},
   "source": [
    "### Step 4: Run Philosopher post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_dir = run_philosopher(ensure_fasta()); workspace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252284b0",
   "metadata": {},
   "source": [
    "### Step 5: Build protein matrix and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df = load_protein_table(workspace_dir); matrix = build_matrix(protein_df); metadata = derive_metadata(matrix); matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Protein matrix saved to: {PROTEIN_MATRIX}')\n",
    "print(f'Metadata saved to: {METADATA_PATH}')\n",
    "print(f'UniProt mapping saved to: {UNIPROT_MAP_PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
